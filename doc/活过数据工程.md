# 活过数据工程

[toc]

------

## 第一章 大数据与数据库

- 大数据的**五维特征**

  - volume：数据量大 对存储要求高
  - velocity：增长速度快，对处理速度和实时性要求高
  - variety：来源和类型多
  - veracity：质量相对较低
  - value：价值密度低

- **为什么大数据火起来了？**

  - 数据收集～
  - 数据处理：计算速度（量子计算）～
  - 数据分析～

- **数据挖掘的价值**：数据 -> 信息 -> 知识

- **🌟大数据处理流程**

  ![image-20210118095529272](活过数据工程.assets/image-20210118095529272.png)

- 大数据**存储**

  <img src="活过数据工程.assets/image-20210118095605237.png" alt="image-20210118095605237" style="zoom: 67%;" />

### 关系数据库

<img src="活过数据工程.assets/image-20210118095748586.png" alt="image-20210118095748586" style="zoom:50%;" />

- 关系模型**完整性**
  - 实体完整性：每条记录有唯一性，通过主码标识
  - 参照完整性：主键和外健
  - 用户自定义完整性
- 主流关系数据库
  - 商用：Oracle、IBM DB2、Microsoft SQL Server、Microsoft Access
  - 开源：MySQL、PostgreSQL
- **DB安全**：用户认证、授权、审计（在数据库上操作进行记录，可以追本溯源）、加密、视图机制
- **🌟事务**：一组相互关联的DB操作，完成一定的业务目标
  - A 原子性
  - C 一致性
  - I 隔离性
  - D 持久性
- **缺点**：严格的数据模式、可扩展性差



### NoSQL

- **优点**：横向扩展；放松一致性约束 数据可以暂时不一致 但最终要一致；容错

- **🌟CAP理论**：C一致性、A系统可用性、P网络分区可容忍性 只能实现两个

  - C：任何时间所有备份有相同的值
  - A：部分节点故障后，集群整体还能提供服务
  - P：系统如果不能在时限内达成数据一致性，则以为发生了分区，必须在C和A之间进行选择

  > CA：关系数据库
  >
  > AP：NoSQL（放宽一致性约束，把数据和处理任务分布到大量节点上运行）

- **分类**

  - 文档DB：以xml、json等文件格式存储数据   🌰MongoDB
  - 列DB：一列所有数据放在一起，可以充分压缩，适合批处理   🌰HadoopDB
  - key-value DB：使用分布式环境，高度可区分    🌰Redis
  - 图DB：利用图形理论储存实体间的关系   🌰Neo4j

  <img src="活过数据工程.assets/image-20210118101013511.png" alt="image-20210118101013511" style="zoom:50%;" />



### 并行与分布式数据库

- **性质**：物理分布性；逻辑完整性；站点自治性

- **区别**

  - 并行：各服务器距离较近，可以共享内存、硬盘等、高速局域网
  - 分布式：通过公共网络连接

  > 分布式DB一般是并行DB，反之不成立

- **信息交换架构**：共享内存、共享磁盘、无共享

  <img src="活过数据工程.assets/image-20210118101459766.png" alt="image-20210118101459766" style="zoom:50%;" />

- **数据划分**：范围、哈希、循环散列

  <img src="活过数据工程.assets/image-20210118101550548.png" alt="image-20210118101550548" style="zoom: 50%;" />

------

## 第二章 数据仓库

- **联机处理架构**

  - OLTP联机事务处理：运行在DB之上；数据实时更新、要求响应性高；面向业务   🌰银行
  - OLAP联机分析处理：运行在数据仓库之上，在大量数据上获得宏观汇总信息

- **特点**

  - 面向主题：要分析什么，就抽象归类什么
  - 集成：从原有分散的DB中集成而来
  - 不更新：只查询DW，不修改数据仓库中的数据
  - 时变

- **系统架构**

  ![image-20210118102452717](活过数据工程.assets/image-20210118102452717.png)

- **🌟ETL**：从DB中抽取、转换、加载data到DW中

  - extract：从不同DB等中抽取数据
  - transformation：相当于数据清洗，解决不一致的问题
  - loading：将清洗过的数据装入目标DW中

  > ETL可以多次使用

- 新技术

  - 多模态DB：多源、异构、跨领域
  - 联邦DB + 同态加密 + 查分隐私保护：数据可用性与隐私保护的矛盾
  - NewSQL DB：综合解决SQL可扩展性差 & NoSQL数据一致性低的问题  🌰TiDB、Google Spanner
  - 内存DB：高性能；数据持久化功能弱  🌰SQLite、SAP HANA



### 数据清洗

- 数据质量评估：正确性、唯一性

- **任务**：格式统一；完整性约束检查；缺失值；元组内部和之间的矛盾；合并重复值；检测离群值

- **🌟数据异常**

  - 语法异常：词法错误、值域格式错误、不规则取值
  - 语义异常：违反完整性约束、数据矛盾、重复值、无效元组
  - 覆盖类异常：值的缺失、元组的缺失

- **方法**

  - 数据分析：检测语法错误  🌰通过edit distance纠正拼写错误

  - 数据转换

    - 标准化：🌰男,女 -> 0,1
    - 规范化：🌰归一化到$[0,1]$区间

  - 重复数据清除

  - 统计方法

    > 离群值 ≠ 错误值



### 数据集成

把数据从多个源整合在一起，提供统一的视图

- **分类**

  - 物理式：从多个数据源 <u>拷贝</u> 到DW中
  - 虚拟式：数据仍保留在数据源中，通过上层抽象，提供视图进行查询

- **集成要解决的问题**

  - 数据管理系统的异构性：🌰Qracle | MySQL
  - 通讯协议的异构性：🌰JDBC | ODBC
  - 数据模式的异构性：🌰SQL | NoSQL
  - 数据类型的异构性
  - 数据取值的异构性：逻辑取值和物理取值
  - 语义的异构性：取值相同但含义不同

- **🌟集成模式**

  1. 联邦数据库：如果仅要在少数几个数据源之间通讯和集成；需要大量的wrapper进行ETL
  2. 数据仓库：完全从DB中复制一份到DW中；增量式更新
  3. 中介者：扮演数据源的虚拟视图，本身不保留data

  <img src="活过数据工程.assets/image-20210118103929352.png" alt="image-20210118103929352" style="zoom:50%;" />

------

## 第三章 聚类

- **目标**：簇内相似性高、距离小；簇间相似性小、距离大

  - 无监督：让聚类自动帮我发现潜在目标
  - 具有不确定性

- **类型**

  - <u>划分聚类</u>(将data划分成非重叠的子集) / <u>层次聚类</u>(嵌套簇的集合组成树)

    <img src="活过数据工程.assets/image-20210118110424534.png" alt="image-20210118110424534" style="zoom:33%;" />

  - <u>独占聚类</u> / <u>非独占聚类</u>(一个点可以在多个簇中)

  - <u>模糊聚类</u>(数据某个簇是0～1间的权重) / <u>非模糊聚类</u>

  - <u>部分聚类</u>(排除离群点和不感兴趣点) / <u>完全聚类</u>

- **聚类形态**

  1. 明显分离：每个簇内对象到簇内对象比到之外近

  2. 基于中心的簇：每个簇内对象到簇的中心比到其他人的中心近

  3. 基于连通的簇：簇内每个点到簇中至少一个点的距离比到其他簇中任意点的距离更近

     - 最近邻、连通性

  4. 基于密度

     > 当簇不规则或互相盘绕，且有噪声和离群点时，通常用density

  <img src="活过数据工程.assets/image-20210118110841267.png" alt="image-20210118110841267" style="zoom:50%;" />



### 距离度量

- **欧几里得距离**：n维空间点中两点的真实距离  <u>如果不同属性尺度不同，要先标准化</u>
  $$
  \operatorname{dis} t=\sqrt{\sum_{k=1}^{n}\left(p_{k}-q k\right)^{2}}
  $$

- 闵可夫斯基距离：将欧式距离泛化到r次方
  $$
  \operatorname{dist}=\left|\sum_{k=1}^{n}(p k-q k)^{n}\right|^{\frac{1}{r}}
  $$

  - r = 1：曼哈顿街区距离
  - r = 2：欧式距离
  - r -> $\infin$：$L_{max}$范数；上确界；以向量任意分量的最大距离代表向量间距离

  > 🌰
  >
  > <img src="活过数据工程.assets/image-20210118111426665.png" alt="image-20210118111426665" style="zoom:50%;" />

- **马氏距离**：考虑相关性
  $$
  \text { mahalanobis }(p, q)=(p-q) \sum^{-1}(p-q)^{T}
  $$

  - $\sum$：输入数据的协方差矩阵

  > 🌰 AC线性相关，距离更近
  >
  > <img src="活过数据工程.assets/image-20210118111627852.png" alt="image-20210118111627852"  />



### 相似度度量

> ❗️这里是点和点的相似度

- 一般属性

  - **一元属性**

    <img src="活过数据工程.assets/image-20210118111901758.png" alt="image-20210118111901758"  />

  - **二元属性**：简单匹配系数；杰卡德系数

    > 🌰
    >
    > <img src="活过数据工程.assets/image-20210118111954843.png" alt="image-20210118111954843" style="zoom:50%;" />

  - **余弦相似度**
    $$
    \cos \left(d_{1}, d_{2}\right)=\frac{d_{1} \cdot d_{2}}{\left\|d_{1}\right\|\left\|d_{2}\right\|}
    $$

    > 🌰
    >
    > <img src="活过数据工程.assets/image-20210118112033695.png" alt="image-20210118112033695"  />

  - **谷本系数(扩展杰卡德)**：针对连续或计数属性
    $$
    T(p, q)=\frac{p \cdot q}{\|p\|^{2}+\|q\|^{2}-p \cdot q}
    $$

- **相关性**：衡量对象间的线性关系

  - 标准化data: $A' = (A - mean(a)) / std(a)$

  $$
  \begin{array}{l}
  p_{k}^{\prime}=\left(p_{k}-\operatorname{mean}(p)\right) / \operatorname{std}(p) \\
  q_{k}^{\prime}=\left(q_{k}-\operatorname{mean}(q)\right) / \operatorname{std}(q) \\
  \text { correlation }(p, q)=p^{\prime} \circ q^{\prime}
  \end{array}
  $$

- **密度**

  - 欧几里得密度：单位体积内点的个数

    <img src="活过数据工程.assets/image-20210118112421522.png" alt="image-20210118112421522" style="zoom:50%;" />

  - 基于中心的欧几里得密度：把方形换成圆形

    <img src="活过数据工程.assets/image-20210118112453890.png" alt="image-20210118112453890" style="zoom:50%;" />

  - 概率密度

  - 基于图的密度



### K-Means聚类

1. 随机初始化k个质心
2. repeat计算每个点到质心的***距离***
3. 将每个点指派到最近的质心，形成k个簇
4. 重新计算每个簇的质心
5. until <u>质心不发生变化</u>｜<u>变化 < 阈值</u>



- **核心**：基于划分；每个点都指派给最邻近质心的簇

- **算法复杂度**：$O(n*k*I*d)$

  - n个点聚为k个类
  - I次迭代
  - d维属性

- **算法评价**：误差平方和SSE（越小越好）
  $$
  S S E=\sum_{i=1}^{K} \sum_{x \in C_{i}} \operatorname{dist}^{2}\left(c_{i}, x\right)
  $$

- **🌟初始质心的选择**

  - 多次运行
  - 预处理
    - 对原始数据抽样，并层次聚类，提取k个簇和他们的质心
    - 选择多余k个的初始质心，在其中选择分布广泛的k个
  - 后处理
    - 对聚类后大的类簇分割、合并（质心最接近的簇、使总SSE增加最小的簇）

- **🌟空簇问题**：某簇只有一个点，它是离群点

  - 预处理：数据归一化；去掉异常值
  - 后处理
    - 选择一个距当前任何质心都最远的点，去掉
    - 将最大SSE的簇继续划分
    - 每个点指派到簇后，随即更新质心（以前都是所有点都指派完才更新质心）
      - 开销大；确保不会产生空簇，可以动态的换SSE目标函数

- **🌟k怎么确定**：二分-K均值算法

  1. 将所有点划分成两个簇：2-means算法
  2. 选大的继续划分
  3. 直至SSE最小

- **局限性**

  <img src="活过数据工程.assets/image-20210118113910740.png" alt="image-20210118113910740" style="zoom:50%;" />



### 层次聚类

1. 计算相似度矩阵
2. 让每个数据点成为一个簇（整体为一个簇）
3. loop until只剩下一个簇（每个数据点成为一个簇）
   1. 合并两个最近的簇（分裂成两个簇）
   2. 更新相似度矩阵



- **类型**：凝聚；分裂

- **优**：不必假定簇数量

- **缺**：没有全局目标函数直接最小化；一旦决定合并不可撤回

- **复杂度**：$O(N^3) \rightarrow O(N^2logN)$    n-1个合并步骤，每一步搜索$n^2$规模的矩阵

- **🌟==簇==之间的相似度**：相似度始终是越大越好

  - 基于最小距离 / 单链：簇的相似性基于两簇中最近的两个点

    - 缺：对噪声和异常值敏感<img src="活过数据工程.assets/image-20210118114717154.png" alt="image-20210118114717154" style="zoom:50%;" />

    
> 🌰
    >
    > <img src="活过数据工程.assets/image-20210121144852725.png" alt="image-20210121144852725" style="zoom: 67%;" />
    
- 基于最大距离 / 完全链接
  
  - 优：不易受噪声和异常值影响
  
  - 缺：倾向于打破大类簇；对球状簇会产生偏差<img src="活过数据工程.assets/image-20210118114734443.png" alt="image-20210118114734443" style="zoom:50%;" />
  
  
  > 🌰
  >
    > <img src="活过数据工程.assets/image-20210118114747109.png" alt="image-20210118114747109" style="zoom:50%;" />
    
  - 基于平均距离 / 平均链接

    - 优：对噪声不敏感
  - 缺：对球状簇会偏差
  
  - 基于Ward方法的簇相似度：簇的相似度基于两簇合并时平方误差的增量

    - 可用作k-means的分层初始化



### DBSCAN基于密度的聚类

1. 消除噪声，对剩余点执行聚类
2. 为距离在Eps内的所有核心点间赋予一条边
3. 每组联通的核心点形成一个簇
4. 将每个边界点指派到一个与之关联核心点的簇中



- **核心**：将核心点连起来  <img src="活过数据工程.assets/image-20210118132045703.png" alt="image-20210118132045703" style="zoom:50%;" />

- **两个超参数**

  - Eps：指定半径内的点数
  - MinPts：满足条件的近邻点数

  > 🔍如何确定Eps和MinPts？
  >
  > 思想：对于簇内的点到第k个近邻点的距离大致相同

- **三类点**

  - 核心点：一个点的邻域内包含的点数高于MinPts
  - 边界点：一个点的邻域内虽然低于MinPts，但该点落在核心点邻域内
  - 噪声点：既不是核心点也不是边界点

- **说明**

  - DBSCAN不是对所有点聚类
  - 算法不完全稳定：某点到两个核心的距离都小于Eps，先聚类的簇会抢到它

- **优**：对异常点不敏感；结果没有偏倚

- **缺**：样本密度不均匀时聚类质量较差；计算量大，收敛较慢；需要对Eps和MinPts联合调参



### 聚类的验证

- **测度/标准**

  - 外部测量：类簇标签和外部类标签的匹配程度（类似有监督）
    - 熵Entropy
  - 内部测量：不参照外部信息
    - 误差平方和SSE
  - 相对测量

- 聚类验证的**可视化**：计算两个矩阵的相关性 <img src="活过数据工程.assets/image-20210118132851871.png" alt="image-20210118132851871" style="zoom:50%;" /> 

  - 相似度矩阵
- 关联矩阵
  

  
- **内部度量方法**

  - SSE

    - 比较两组簇：K相同时，SSE越小越好
    - 比较两个簇：平均SSE越小，类簇越紧实
    - 出现拐点处适合作为K的个数

  - 内聚和分离

    - 内聚：簇内的误差平方和 WSS
    - 分离：簇间的距离平方和 BSS

    > WSS + BSS = 常数

  - 轮廓系数：$s = 1-\frac{a}{b} \ if \ a < b, \ else \ \frac{b}{a} - 1$

    - a: 每个点i到其所在簇内其他点的平均距离
    - b: min(i到其他簇中点的平均距离)
    - 取值范围$[0,1]$，越接近1越好



### Chameleon动态建模聚类(略)

- **核心思想**：适应数据集的特征来寻找自然簇

- **基于图的聚类算法**；簇是图的连通分量；稀疏化

- **衡量标准**：簇间相对接近度；簇间相对互联度

  - 如果二者达到一定阈值，则进行合并，这种方案保持了自相似性

  <img src="活过数据工程.assets/image-20210118133759633.png" alt="image-20210118133759633" style="zoom:50%;" />



### 聚类算法比较表格

![image-20210118133945768](活过数据工程.assets/image-20210118133945768.png)

------

## 第四章 分类

- **大数据分析分类**
  - 有监督分析
  - 无监督分析
  - 分类
  - 回归（数值预测）：根据所给data建立回归模型，推测未知或缺失的data
- **分类过程**：train  -->  validation  -->  test
  - valid：在独立的验证集上验证，防止过拟合



### 决策树

由数据的不同属性逐次划分dataset，直至得到的数据子集只包含同一类数据为止，这样得到的树成为决策树

<img src="活过数据工程.assets/image-20210118150743655.png" alt="image-20210118150743655" style="zoom:50%;" />

- 由树的根到某个叶属性的合取可形成一条分类规则
- 所有规则的析取形成一整套分类规则（什么条件下得到什么值的规则）

<img src="活过数据工程.assets/image-20210118150926377.png" alt="image-20210118150926377" style="zoom:30%;" />

- **🌟决策树生成算法**：自上而下递归的分治法构建树
  - 开始时所有训练样本都在根
  - 用一定的模型***<u>选择属性</u>***生成分支
    - 模型：试探｜统计   🌰信息增益、熵
    - 一般树越小，预测能力越强
  - 停止分裂
    - 对于一个给定的节点，所有的样本属于同一类
    - 没有剩余属性用于进一步区分，则分类叶子采用多数投票方式
  - 直至所有样本都被计算
- **🌟决策树修建算法**：防止噪声和过拟合
  - 预剪枝：在生成树的同时决定是否继续对不纯的训练子集划分还是停止
  - 后剪枝：拟合-化简的两阶段方法
    - 首先构建一棵train完全拟合的树
    - 使用一个测试集(tuning set)从叶开始剪枝，如果某个叶子剪去后测试集上的准确度不降低，则剪去改叶子

#### CLS算法

早期决策树的基本框架，但并没规定属性选择的方法

<img src="活过数据工程.assets/image-20210118151639251.png" alt="image-20210118151639251" style="zoom: 67%;" />

#### ID3算法

- **🌟熵**

  - 熵越小，不纯度越小，不确定信息越小，越有利于数据分类
  - 决策树的分支原则是使划分后样本的子集熵最小

  $$
  \operatorname{entapy} (S)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}
  $$

  - 参数含义
    - train set $S$
    - $n$个类别样本，类别分别为$C_i$
    - 属于类$C_i$的概率为$p_i$
  - 如果将n类样本看成n种不同的消息，则熵表示对每一种消息编码所需的平均比特数
    - $|S|\times entropy(S)$为对S进行编码所需的比特数；$|S|$为样本数目
    - $p_i = \frac{1}{n}$时，$e(S) = log_2 n$  ==样本概率分布均匀，混杂度高，信息熵大==
    - 所有样本属于同一类时，$e(S) = 0$

- **🌟信息增益**：衡量熵期望的减少值

  - 信息增益越大，因为知道A导致熵的压缩越大，A对分类提供的信息越多

  - ==选择获得最大信息增益的属性作为分支属性==
    $$
    gain(S,A) = entropy(S) - entropy(S,A) \\
    \operatorname{entropy}(S, A)=\sum_{i=1}^{m} \frac{\left|S_{i}\right|}{|S|} \operatorname{entropy} \left(S_{i}\right)
    $$

    - 设属性A将S划分成m份
    - $S_i$为根据属性A划分S的第i个子集

- **基本策略**：使用信息增益作为启发知识帮助选择合适的属性将样本分类

- 算法

  <img src="活过数据工程.assets/image-20210118153633341.png" alt="image-20210118153633341" style="zoom:50%;" />

- **递归停止条件**

  - 给定节点的所有样本属于同一类
  - 没有剩余属性可用来进一步划分样本，这时候该节点作为树叶，并用剩余样本中出现最多的类型作为叶节点的类型
  - 某一分支没有样本，则以训练集种占多数的类型创建一个树叶

- **评价**

  - 优：容易理解；噪声影响较小；分类速度快
  - 缺：非递增算法；结果非全局最优
    - 倾向于选择有大量不同取值的属性，产生小而纯的子集

#### C4.5算法

- **🌟增益比例**：信息增益的标准化
  $$
  gain\_ratio(S,A) = \frac{gain(S,A)}{I(S,A)}
  $$

  - 信息量 $I(S,A)$
    $$
    I(S, A)=-\sum_{i=1}^{m} \frac{\left|s_{i}\right|}{|s|} \log _{s} \frac{\left|s_{i}\right|}{|s|}
    $$

  - 样本S在属性A上的取值分布越均匀，信息量越大

  - 用来衡量属性分裂数据的广度和均匀性

- **算法特点**

  - 分支指标采用增益比例
  - 将处理能力扩充到数值属性
    - 按数值属性的大小对样本排序，每次划分数值属性的取值范围
    - 每一个数值属性划分为两个区间：大于阈值｜小于等于阈值
  - 处理缺少属性值的训练样本
    - 最常用值代替
    - 该属性所有取值的平均值代替
    - 概率填充
  - 使用K次迭代交叉验证

- 算法

  <img src="活过数据工程.assets/image-20210118153701456.png" alt="image-20210118153701456" style="zoom:50%;" />

- **特点**

  - 优：准确率高；分类规则易于理解
  - 缺：效率低（构建树时要对dataset进行多次顺序扫描和排序）

#### CART算法

- **🌟Gini基尼不纯度指标**
  $$
  \operatorname{Gini}(p)=\sum_{k=1}^{k} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{k} p k^{2}
  $$

  - p(选中) * p(分错)
  - 表示一个随机选中的样本在子集中被分错的可能性
  - ==Gini越小，数据越纯，不确定性越小==

- **算法特点**

  - 二元划分：二叉树不易产生数据碎片，精度高
  - 不纯性度量：
    - 分类目标：Gini指标
    - 连续目标：最小平方残差、最小绝对残差
  - 剪枝：用独立的dataset对训练集的生成树剪枝，防止过拟合

- 算法

  <img src="活过数据工程.assets/image-20210118154222991.png" alt="image-20210118154222991" style="zoom:50%;" />

- **CART剪枝**

  - 在预测误差和树复杂度间找到平衡
  - 极小化决策树整体的损失函数或代价函数实现

  > 首先从生成算法产生的决策树T0底端开始不断剪枝，直到T0的根结点,形成一个子树序列[(TO,T1....Tn)
  > 然后通过交叉验证法在独立验证数据集上对子树序列进行测试，从中选择最优子树

- **特点**

  - 优：计算少速度快，非参数方法
  - 缺：容易过拟合；性能和样本有较大关系

------

### SVM支持向量自动机

- 背景理论（略）

  <img src="活过数据工程.assets/image-20210118155548859.png" alt="image-20210118155548859" style="zoom: 33%;" />

  - 经验风险最小 =/=> 期望风险最小
  - 样本数量越多，置信风险越小，越有可能学习正确
  - 分类函数VC维越大，置信风险越大，推广能力越差
  - ==提高样本数量，降低VC维，降低置信风险==

- SVM建立在VC维理论和结构化风险最小原理基础上

  - 结构化风险 = 经验风险 + 置信风险
    - 经验风险：分类器在给定样本上的误差
    - 置信风险：分类器在未知样本上分类结果的误差

- **核心思想**：寻找一个满足分类要求的超平面，并且使训练集中的点距离分类面尽可能的远  ==（寻找一个分类面使两侧的空白margin最大）==

- **线性SVM**

  - **核心思想**：寻找决策平面 $w^Tx + b = 0$，使训练样本中正负类输入分别位于超平面两侧，且

    - 经验风险最小（错分最少）
    - 推广能力最大（空白最大）

    > 求一个平面使两类被分开且间隔最大 <img src="活过数据工程.assets/image-20210118175638190.png" alt="image-20210118175638190" style="zoom:50%;" />
    >
    
- **优化函数**： $min\frac{||w||^2}{2}$
  
- 采用拉格朗日乘子法
  
- 改进：加入松弛因子，用广义分类面求解
  
- 算法步骤：略
  
- **非线形SVM**

  - **核心思想**：用非线性映射将原始数据变换到高维特征空间，在高核空间用线性SVM求解

  - 核函数：要能真实的反应训练样本的远近关系

    <img src="活过数据工程.assets/image-20210118175905963.png" alt="image-20210118175905963" style="zoom:50%;" />

  - **特点**

    - 决策函数只由少数的支持向量确定，与样本空间维数无关，避免“维数灾难”
    - 抓住关键，剔除冗余

------

### 贝叶斯分类器

用概率统计的方法研究决策问题，根据每一类总体的概率分布决定未知类型的样本

- **前提要求**

  - 各类别的总体概率分布是已知的
  - 要决策分类的类别是一定的

- **概率论背景知识**

  <img src="活过数据工程.assets/image-20210118180335527.png" alt="image-20210118180335527" width="80%;" />

- ==用后验概率分类$p(w_i|x)$==

- 两类Bayes决策

  - **最小错误率**

    - **决策规则**：$max_{j=1...c} \ \ p(w_j|x)$
    - 误差概率：$min[\ p(w_i|x) \ ]$

  - **最小风险**：对样本的分类不仅要尽可能给出正确的判断，而且还要考虑到做出错误判断的后果，把<u>分类错误的损失</u>加入到决策中去

    - **三个空间**：状态空间$w$ / 决策空间$a$ / 损失空间$\lambda(a,w)$

      - 用决策表或损失矩阵表示三者的关系  <img src="活过数据工程.assets/image-20210118181351045.png" alt="image-20210118181351045" style="zoom:50%;" />

    - **两种风险**

      - 条件风险：条件期望损失，即后验概率加权和
    $$
        R\left(a_{i} \mid x\right)=E[\lambda_{\left(a_{i}, h_{j}\right)}]=\sum_{i=1}^{c} \lambda\left(a_{i}, w_{j}\right) P\left(w_{i} \mid x\right).
        $$
    
      - 期望风险：反映对整个特征空间所有x的取值都采取相应的政策$a(x)$所带来的平均风险
    $$
        R=\int R(a(x) \mid x) p(x) d x
        $$
    
    - **决策规则**：如果采取每个决策$a_i$使条件风险$R(a_i|x)$最小，则对所有x作决策时期望风险R也必然最小
  $$
      \begin{array}{l}
      R\left(a_{1} \mid x\right)=\lambda_{11} p\left(w_{1} \mid x_{1}\right)+\lambda_{12} p\left(\omega_{3} \mid x\right) \\
      R \left(\operatorname{a}_{2} \mid x_{1}\right)=\lambda_{21} p\left(w_{1} \mid x\right)+\lambda_{22} p\left(c_{2} \mid x\right) \\
      \text { 取 } \min \left\{R\left(a_{1} \mid x\right), R\left(a_{2} \mid x\right)\right\}
      \end{array}
      $$
  
  > 🌰
>
  > <img src="活过数据工程.assets/image-20210118181802834.png" alt="image-20210118181802834" style="zoom:50%;" />
  
  - **两类Bayes决策的关系**
  - 若正确决策没有损失，错误决策损失均为1，则最小错误率 == 最小风险
    - 若损失对称，则二者方法相同 $\lambda_{12}-\lambda_{22}=\lambda_{21}-\lambda_{11}$
  
- 正态分布下的Bayes决策（略，完全看不懂）

  <img src="活过数据工程.assets/image-20210118182259961.png" alt="image-20210118182259961" style="zoom:50%;" />

------

## 第五章 神经网络

### 浅层神经网络

**【分类】**

- 学习方式：有导师 / 强化学习 / 无导师
- 网络状态：连续性 / 离散型
- 网络结构：前向网络（分层结构） / 反馈网络（互连网络）
- 网络的活动：确定性网络 / 随机性网络  🌰 玻尔兹曼机、高斯机

#### M-P神经元模型

<img src="活过数据工程.assets/image-20210118195435890.png" alt="image-20210118195435890" style="zoom:50%;" />

- 树突：输入信号（多）
- 细胞核：计算
- 轴突：传输
- 突触：输出（单）

<img src="活过数据工程.assets/image-20210118195556259.png" alt="image-20210118195556259" style="zoom:50%;" />

- $x_1...x_n$：神经元的n个输入信号量
- $w_i...w_n$：对应输入的权值（信号源与该神经元的连接强度）
- $\sum(U)$：神经元的输入综合（相应于生物神经细胞的膜电位，成为激活函数）
- $y$：神经元的输出
- $b(\theta)$：神经元的阈值



**【常见的激活函数】**

- 阈值型：电位大于某个阈值时激活；否则神经元不激活
- S型：反映神经元的非线形输出特征
- 分段线性型

<img src="活过数据工程.assets/image-20210118200002486.png" alt="image-20210118200002486" style="zoom:50%;" />



#### 单层感知机模型ANN

- 激活函数采用阈值型

- 能实现布尔逻辑(二分类)，但只能线性分类

- **算法**

  <img src="活过数据工程.assets/image-20210118200203265.png" alt="image-20210118200203265" style="zoom:75%;" />

  - 循环终止条件
    - 循环次数控制法
    - 精度控制法
  - 调整权重
    - 最小均方误差LMSE
    - 梯度下降规则 delta
  - 训练方式
    - 增量式
    - 批处理式

  > 单层感知机无法解决「异或」的问题



#### B-P网络模型

- **核心**：有监督；误差反向传播

- **特点**

  - 优：广泛的实用性和有效性
  - 缺：训练速度慢（循环计算）；局部极小点的逃离问题（随机权值）；算法不一定收敛（参数影响很大）；接受样本的顺序对学习有较大影响（更偏向较后出现的样本）

- **结构**

  <img src="活过数据工程.assets/image-20210118200717927.png" alt="image-20210118200717927" style="zoom:50%;" />

- **学习过程**

  - 前馈计算（求解误差）：逐层逐单元前馈计算
    - 通过简单非线形函数的复合可得到复杂非线性映射能力
  - 反馈计算（误差反向传播）：梯度下降的反方向
  - 计算每个权重的调整量

- **分析与改进**

  - 克服收敛慢、陷入误差局部极小：反向传播时，在每个权重变化上加入一项动量项，利用附加栋梁作用滑过极小值

    <img src="活过数据工程.assets/image-20210118201500643.png" alt="image-20210118201500643" style="zoom:50%;" />

  - 样本：训练集、检验样本（10%以上）、测试样本（10%以上）

    - 经验上 |training set| > 5 |set of weights|

  - 提高泛化性：因为追求过小的误差，使网络对训练样本点学习太精致，以至于记住的不是训练样本的一半特征而不是局部特征

    - 适当的训练精度  🌰将grand truth从1,0 --> 0.99,0.01
    - 提前停止
    - 给样本增加随机噪声  🌰(x1,x2) --> (x1+e1, x2+e3)

  - 隐藏层数量：越多越精确；但容易过拟合、开销大

  - 输入信号归一化：训练时会出现饱和现象，即增加输入数量，但输出不变

    - 应使所有样本输入平均值接近零或与方差相比很小
    - 均值平移 -> 去相关PCA -> 协方差均衡

  - 权重初始化：建议在(-2.4/F, 2.4/F)内随机初始化；F为输入单元数

  - 激励函数：常用双曲正切

------

### 深度神经网络

- **发展基础**：数据爆炸；计算性能大幅提升



#### 深度信念网络DBN

- 无监督

- **概率生成模型**：底层通过限制<u>玻尔兹曼网络</u>（层间无连接；层内有连接）让整个网络按最大概率<u>生成训练数据</u>

- 常将多层玻尔兹曼网络后叠加BP网络

  - 也可看成是对深层BP网络权值的初始化

- 对比散列CD

  - 开始时用训练数据初始化可见层，然后用条件分布计算隐层
  - 再根据隐层用条件分布计算可见层，产生对结果的一个重构

  <img src="活过数据工程.assets/image-20210118202832382.png" alt="image-20210118202832382" style="zoom:50%;" />



#### 卷积神经网络CNN

- **特点**

  - 神经元间是非全连接的
  - 同一层中某些神经元间连接的权重是共享的（减少参数）

- **网络结构**

  <img src="活过数据工程.assets/image-20210118204050813.png" alt="image-20210118204050813" style="zoom:50%;" />

  - **输入层**：二维矩阵（黑白图像）；三维矩阵（RGB）

  - **卷积层**：用于提取特征，复合卷积核特征的神经元被激活

    $h(x)=f(x) * g(x)=\int_{-\infty}^{+\infty} f(t) g(x-t) d t$

    - 卷积核：一个filter上对应像素的权重值

    - 步长：卷积核在图像上一次移动的步长

    - 局部连接：前一层局部神经元经过卷积连接下一层的*<u>一个</u>*神经元

    - 平移不变性：前一层不同位置的像素共享卷积核权重，改变对象位置只改变下层激活神经元的位置，而不影响存在性

      > 图像识别中，同一类别的图像往往只有细微差别，局部平移不变性大大提高了图像分类的准确性

  - **池化层**：下采样（对feature map提取再加工）

    - 又一次提取特征，减少参数，提高计算效率
    - 种类
      - 最大池化：增强局部特征（提取轮廓）
      - 平均池化：减少噪声（模糊图像）
    - 提高局部平移不变性，对输入的微小变化产生更大的容忍
    - 进一步获取更抽象的信息，防止过拟合

  - **全连接层**：分类器，用学到的特征进行分类

    <img src="活过数据工程.assets/image-20210118204109558.png" alt="image-20210118204109558" style="zoom:50%;" />

    - 激活函数：softmax归一化指数函数，将输出映射到[0,1]

      <img src="活过数据工程.assets/image-20210118204201183.png" alt="image-20210118204201183" style="zoom:50%;" />

- **超参数**

  - 学习率：训练时更新权重的幅度
  - 层数：与特征提取有关
  - 卷积层：卷积核大小、数量、步长、填充、激活函数
    - 输出通道数 = 卷积核数
  - 池化层：采样大小
    - 输出通道数 = 输入通道数

- **分析与改进**

  - 梯度消失问题：对sigmoid函数求骗到得到的数值偏小，层数过多时，梯度更新几乎为0

  - 局部感知与权值共享

    <img src="活过数据工程.assets/image-20210118204404682.png" alt="image-20210118204404682" style="zoom:50%;" />

  - 填充padding：保持边界信息

  - 归一化：解决过拟合

    - 损失函数添加L1、L2范数正则化
    - batch normalizatioin
    - Dropout：随即将某些权值不更新  $\delta \in (0,1)$
      - 训练时随机忽略$\delta$比例的神经元，只更新剩下神经元的权值
      - 检测时使用全部连接，每个神经元 * $\delta$

  - 特征可视化：低层提取简单特征；高层提取复杂特征

    <img src="活过数据工程.assets/image-20210118205048358.png" alt="image-20210118205048358" style="zoom:50%;" />

- 成熟CNN模型：LeNet、AlexNet、NIN、GoogLeNet、残差ResNet



#### 循环神经网络RNN

引入**“记忆”**概念，对<u>序列数据</u>建模。预测不只依赖于input，还依赖于之前的一部分信息

<img src="活过数据工程.assets/image-20210118205506288.png" alt="image-20210118205506288" style="zoom:50%;" />

- 同层间的信息在时序上流动
- RNN中共享一组参数，减少参数量
- 🌰 自然语言处理；视频；气象观测数据；股票数据
- **双向RNN**：不仅依赖之前的序列，还可能依赖后面的  🌰文本补全
- **BPTT基于时间的反向传播**：引入时序演化，将整个序列作为一次训练，要求每个时刻的误差求和
  - 问题：梯度消失；梯度爆炸
  - 解决方案：激活函数更新 ReLU
  - 改进网络结构 LSTM

------

## 第六章 数据可视化

- **领域分支**

  - 科学可视化：物理世界里采集的
  - 信息可视化：对抽象数据的直观展示
  - 可视分析

- **可视化评估**

  <div align="left"><img src="活过数据工程.assets/image-20210118191815872.png" alt="image-20210118191815872" style="zoom:50%;" /></div>

  - **t检验**：评估两组数据的均值是否存在统计学上的显著差异
    - 单一样本t检验：将总体的平均值与理论值进行比较
    - 非配对双样本t检验：比较两个独立样本的均值（组间）
    - 配对t检验：两组相关样本的均值（组内）



### 信息可视化模型

抽象数据 -> 数据结构 -> 可视化

<img src="活过数据工程.assets/image-20210118184441082.png" alt="image-20210118184441082"  />

- **数据处理**
  - 数据清洗：去除噪声，提取有用信息  🌰过滤、取样、压缩、分类、聚类等
  - 数据变换：改变存储形式  🌰无结构data -> 有结构
  - 数据抽象：🌰社交网络 -> 图
- **可视化设计**
  - 信：准确表达数据，不产生偏差和歧义
  - 达：高效传递数据中的信息
    - 可视化编码：如何用<u>图形符号</u>及<u>属性</u>展示数据元素
      - 符号：点、线、面
      - 属性：颜色、形状、大小
  - 雅：美观
- **可视化布局**：通过算法自动计算元素在屏幕上的位置，**优化过程**   🌰避免点的遮挡、线的交叉
- **可视化渲染**
- **可视化交互与动画**



### 可视化设计准则

- **信**

  - 控制图形中的<u>谎言因子</u>

    - 衡量可视化中所表达的数据量与数据之间夸张程度的度量
      $$
      LF = \frac{\text{可视化中元素的相对变化量}}{\text{数据的真实变化量}}
      $$

    - 一般要求LF位于0.95~1.05之间

  - 不要将数据的展示脱离其上下文

  - 展示数据变换而非设计变化

- **达**

  - 最大化<u>数据墨</u>水占比

    - 可视化图形中不可被擦出的核心部分
      $$
      \text{Data-Ink Ratio} = \frac{\text{数据墨水}}{\text{绘制中的所有墨水}}
      $$

    - 擦出非数据墨水   🌰但如刻度坐标轴有很大含义

    - 擦出冗余数据墨水   🌰但如对称图形不好只保留一半

  - 利用人类感知系统的特点，更快的感知信息

    - 减少感知可视化信息的耗时
    - 减少模式识别过程的耗时

  - **格式塔**

    <img src="活过数据工程.assets/image-20210118185922412.png" alt="image-20210118185922412" style="zoom:50%;" />

  - **🌟可视化编码**

    - 可视化符号：元素和元素之间的关系
      - 元素：点、线、面
      - 关系：闭包、连线
    - 可视化通道：数据属性、控制符号的展现形式
      - 数值通道：🌰*<u>位置</u>*、长度、角度、色温 etc.
      - 表示通道：🌰*<u>空间区域</u>*、色相、动向、形状

- **雅**：审美问题（这里略）



### 交互技术

- **常见交互类型**

  - 选择：标记感兴趣点并追踪其变化  🌰地图上标及地点
  - 导航：展示数据的不同子集
  - 重构：改变空间布局以提供不同视角
    - 重组视图 / 排序
  - 编码：视觉外观 颜色、大小、图表样式等
  - 抽象 / 具象：层次结构
  - 过滤：基于限定条件展示数据
    - 动态查询 / 刷选直方图
  - 关联：高亮显示data间relationship
    - 刷选 & 连接

- **常见交互范式**

  - **概览 + 细节**：用户先对整体有个印象，如果想深入再查看细节
    - 滚动
    - 缩放和平移
  - **焦点 + 上下文**：显示焦点的细节信息 + 焦点周边的信息
    - 可变形形变
      - 压缩不重要的部分，突出重点
      - 行变函数 / 放大函数

- **可视化动画**

  - 作用

    - 帮助理解数据：展示中间步骤和过渡
    - 提升用户关注度：提供新视角，辅助用户更深入的查看数据
    - 关联视图：在两个视图间平滑过度

  - 实现

    <img src="活过数据工程.assets/image-20210118191516865.png" alt="image-20210118191516865" style="zoom:50%;" />



### 多维数据可视化

- **数据的维度**：用于描述data的各种属性

- **🌟数据维度的类型**

  - 分类属性
  - 有序属性
  - 数据属性

- **数据之间的差异性**

  - 差异阵：每个取值代表元素之间的数据距离

- **🌟坐标系｜可视空间**

  <img src="活过数据工程.assets/image-20210118192026650.png" alt="image-20210118192026650" style="zoom: 67%;" />

  - **正交投影空间**

    <img src="活过数据工程.assets/image-20210118192020118.png" alt="image-20210118192020118" style="zoom:50%;" />

  - **非正交投影空间**

    - MDS：相似度结构分析
    - ISO Map：用测地距离反映元素之间的相关性

  - **平行坐标轴系统**：每一个轴代表一个维度

    - 消除视觉混乱（维度太多）
      - 基于数据过滤和聚类
      - 基于坐标轴排序优化
      - 基于视觉增强

    <img src="活过数据工程.assets/image-20210118192119000.png" alt="image-20210118192119000" style="zoom: 67%;" />

- **树的可视化**：层次化data

  - 点线图
  - 邻接图
  - 包含图

  <img src="活过数据工程.assets/image-20210118192459355.png" alt="image-20210118192459355" style="zoom:50%;" />

- **图的可视化**

  - 点线图：布局问题 / 角度问题
    - 力导向布局：迭代求解，使点与点间相互位置的势能降到最低
      - 可以用弹簧模型 用距离模拟力，当力平衡时势能最小
  - 邻接矩阵
  - 混合

  <img src="活过数据工程.assets/image-20210118192618492.png" alt="image-20210118192618492" style="zoom: 67%;" />

